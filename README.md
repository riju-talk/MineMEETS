# MineMEETS ‚Äî Multimodal RAG Meeting Intelligence Platform (MLOps-Focused)

MineMEETS is a **production-style multimodal RAG system** for processing and retrieving meeting intelligence across **text, audio/video, and visual content**, with an emphasis on **reliable pipelines, vector infrastructure, and operational concerns**.

The project demonstrates **end-to-end ML system deployment practices** using **Pinecone**, Whisper, CLIP, and a locally hosted LLM runtime.

---

## üéØ Project Intent (Very Important)

> Focus on **operational ML pipelines**, system reliability, and data flow ‚Äî
> **not model innovation or research contributions**.

This is an **MLOps / ML Systems Engineering** portfolio project demonstrating:

* Production-grade ingestion pipelines
* Vector database operations and management
* Deterministic preprocessing with validation
* Stateless retrieval services
* Operational observability and monitoring
* Container-based deployment patterns

---

## üß† Core Capabilities

### Ingestion Pipelines (Operational Focus)

* Batch ingestion of:
  * Text transcripts (.txt, .pdf, .docx)
  * Audio/video files (.mp3, .wav, .m4a)
  * Images/screenshots (.png, .jpg, .jpeg)
* Deterministic preprocessing with validation and fallback paths
* Idempotent processing per `meeting_id`
* Dimension validation for embeddings
* Batch upsert with configurable sizes

---

### Feature Engineering & Embeddings

* **Text embeddings**: Sentence Transformers (CLIP ViT-B/32)
* **Audio ‚Üí Text**: Whisper transcription, then embedded
* **Image embeddings**: CLIP ViT-B/32 visual encoder
* **Unified embedding interface** with strict dimensional checks (512-dim)
* Preprocessing includes chunking with configurable overlap

---

### Vector Infrastructure (Pinecone)

* **Namespace-per-meeting isolation** enables:
  * Per-meeting reprocessing without affecting others
  * Selective deletion and rollback
  * Cost-controlled operations
* **Metadata-first schema design** for:
  * Semantic similarity search
  * Modality-aware retrieval (text/audio/image)
  * Time-range filtering
  * Debugging and auditability

---

## üèóÔ∏è System Architecture (MLOps View)

```
Raw Inputs (Text/Audio/Images)
   ‚Üì
Ingestion Jobs (Validation & Routing)
   ‚Üì
Preprocessing & Chunking (Deterministic)
   ‚Üì
Embedding Workers (Whisper/CLIP/SentenceTransformer)
   ‚Üì
Vector Store (Pinecone with Namespaces)
   ‚Üì
Retrieval Service (Hybrid Search)
   ‚Üì
LLM Inference (Ollama - Local)
   ‚Üì
Gradio UI (Thin Client)
```

**Each stage is:**
* Independently testable
* Restartable without side effects
* Observable with logging and metrics

---

## üîß Technology Stack

| Layer              | Tool                   | MLOps Reasoning                     |
| ------------------ | ---------------------- | ----------------------------------- |
| Language           | Python 3.10+           | ML ecosystem standard               |
| Orchestration      | Explicit pipelines     | Predictable execution               |
| Vector DB          | Pinecone               | Managed scaling & reliability       |
| Audio Processing   | Whisper                | Deterministic transcription         |
| Vision Processing  | CLIP ViT-B/32          | Stable multimodal embeddings        |
| LLM Runtime        | Ollama                 | Local inference control             |
| UI                 | Gradio                 | Simple production-ready interface   |
| Containerization   | Docker                 | Reproducible deployments            |
| CI/CD              | GitHub Actions         | Automated testing and builds        |
| Code Quality       | Black, Pylint, Pytest  | Maintainable, tested codebase       |

---

## üì¶ Pinecone Index Design (Operational)

### Namespace Strategy

* `meeting_id` = namespace
* **Enables:**
  * Per-meeting reprocessing
  * Safe rollback of bad data
  * Cost-controlled deletion
  * Isolation for multi-tenant scenarios

### Metadata Schema

```json
{
  "meeting_id": "meeting_20260131_143022",
  "modality": "text | audio | image",
  "type": "text_chunk | audio_segment | image_embed",
  "source": "transcript | whisper | screenshot",
  "chunk_id": "meeting_20260131_143022_chunk_14",
  "chunk_index": 14,
  "timestamp_start": 120,
  "timestamp_end": 145,
  "position": 14,
  "total_chunks": 47
}
```

**Used for:**
* Filtered retrieval by modality or time range
* Debugging incorrect answers
* Audit trails and compliance
* Performance monitoring

---

## üîç Retrieval Layer

* **Hybrid search strategies:**
  * Semantic similarity via vector embeddings
  * Keyword-based search for better recall
  * Query expansion for general questions
* **Metadata filtering** for modality and temporal constraints
* **Deterministic ranking logic** (no stochastic agent behavior)
* **Deduplication** and score normalization

> Retrieval is treated as a **service**, not an experiment.

---

## üí¨ Inference & Serving

* **Context assembly** with:
  * Token limit constraints
  * Modality indicators for cross-modal reasoning
  * Source attribution
* **LLM served locally** via Ollama HTTP API
* **Stateless Q&A execution** (easy to containerize and scale)
* **No external API dependencies** (privacy-preserving)

---

## üìä Observability & Reliability

Implemented operational hooks:

* ‚úÖ **Ingestion logging**: Success/failure per meeting
* ‚úÖ **Embedding validation**: Dimension checks before upsert
* ‚úÖ **Pinecone upsert counts**: Per-job metrics
* ‚úÖ **Retrieval latency**: Tracked per query
* ‚úÖ **Graceful fallbacks**: On partial pipeline failures
* ‚úÖ **Error logging**: Structured logs with context
* üîÑ **Metrics collection**: (Planned for monitoring dashboards)

---

## üîÅ Reprocessing & Maintenance

* ‚úÖ **Full meeting re-ingestion supported**
* ‚úÖ **Selective modality reindexing** (e.g., text-only, audio-only)
* ‚úÖ **Safe deletion via namespace purge**
* ‚úÖ **Idempotent operations** (running twice produces same result)

This is **classic MLOps hygiene**.

---

## üöÄ Deployment Model

Designed to run:

* ‚úÖ **Locally** for development and testing
* ‚úÖ **In Docker** for reproducible environments
* ‚úÖ **As batch jobs** + API service for production
* ‚úÖ **No hard dependency on UI** (can run headless)
* ‚úÖ **LLM runtime isolated** from ingestion pipeline

---

## ‚ùå Explicit Non-Goals (MLOps-Correct)

* ‚ùå Model fine-tuning or training
* ‚ùå Novel architectures or research
* ‚ùå Research benchmarks or leaderboards
* ‚ùå Autonomous agents with complex planning
* ‚ùå Overlapping orchestration frameworks (e.g., Airflow, Prefect)

---

## üöÄ Quick Start

### Prerequisites

* Python 3.10+
* Docker (optional, for containerized deployment)
* Ollama installed and running locally
* Pinecone account and API key

### 1. Install Dependencies

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

Create a `.env` file:

```env
# Pinecone
PINECONE_API_KEY=your-pinecone-api-key-here

# Whisper
WHISPER_MODEL=base
WHISPER_CACHE_DIR=.cache/whisper

# Ollama
OLLAMA_MODEL=llama3.1
OLLAMA_HOST=http://localhost:11434
```

### 3. Start Ollama

```bash
# Download and start Ollama from https://ollama.com/download
ollama pull llama3.1
ollama serve  # Runs on http://localhost:11434
```

### 4. Run Application

```bash
# Using Make (recommended)
make run

# Or directly with Python
python app.py
```

### 5. Use the Interface

* Open browser to `http://localhost:7860` (Gradio default)
* Upload meeting files (text, audio, images)
* Click "Process Meeting"
* Ask questions in the Q&A tab

---

## üê≥ Docker Deployment

### Build and Run

```bash
# Build image
make docker-build

# Run container
make docker-run

# Or use docker-compose
docker-compose up --build
```

### Environment Variables

Pass environment variables via `.env` file or docker-compose:

```yaml
environment:
  - PINECONE_API_KEY=${PINECONE_API_KEY}
  - OLLAMA_HOST=http://host.docker.internal:11434
```

---

## üõ†Ô∏è Development

### Code Quality

```bash
# Format code
make format

# Lint code
make lint

# Run tests
make test

# Run all quality checks
make check
```

### Project Structure

```
MineMEETS/
‚îú‚îÄ‚îÄ agents/                  # Core pipeline modules
‚îÇ   ‚îú‚îÄ‚îÄ audio_agent.py       # Whisper transcription
‚îÇ   ‚îú‚îÄ‚îÄ image_agent.py       # CLIP image embeddings
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py # Text chunking
‚îÇ   ‚îú‚îÄ‚îÄ pinecone_db.py       # Vector operations
‚îÇ   ‚îú‚îÄ‚îÄ multimodal_rag.py    # Retrieval logic
‚îÇ   ‚îú‚îÄ‚îÄ qa_agent.py          # Q&A orchestration
‚îÇ   ‚îú‚îÄ‚îÄ llm.py               # LLM interface
‚îÇ   ‚îî‚îÄ‚îÄ coordinator.py       # Pipeline coordinator
‚îú‚îÄ‚îÄ tests/                   # Unit and integration tests
‚îú‚îÄ‚îÄ data/                    # Data storage
‚îÇ   ‚îî‚îÄ‚îÄ raw/                 # Input files
‚îú‚îÄ‚îÄ app.py                   # Gradio UI application
‚îú‚îÄ‚îÄ requirements.txt         # Production dependencies
‚îú‚îÄ‚îÄ pyproject.toml           # Project metadata & dev deps
‚îú‚îÄ‚îÄ Dockerfile               # Container definition
‚îú‚îÄ‚îÄ docker-compose.yml       # Multi-container orchestration
‚îú‚îÄ‚îÄ Makefile                 # Operational commands
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml           # CI/CD pipeline
‚îî‚îÄ‚îÄ README.md                # This file
```

---

## üìà CI/CD Pipeline

GitHub Actions workflow (`.github/workflows/ci.yml`):

* ‚úÖ **Lint**: Black, Pylint checks
* ‚úÖ **Test**: Pytest with coverage
* ‚úÖ **Build**: Docker image creation
* ‚úÖ **Validate**: Type checking with MyPy

Runs on:
* Every push to `main`
* All pull requests

---

## üìå Resume-Ready Description (MLOps Version)

**MineMEETS ‚Äî Multimodal RAG Meeting Intelligence Platform**

* Built an end-to-end MLOps-oriented pipeline for ingesting, embedding, and retrieving meeting data across text, audio, and images
* Designed Pinecone-backed vector infrastructure with namespace isolation, metadata filtering, and safe reindexing workflows
* Integrated Whisper and CLIP into deterministic embedding pipelines with validation and fallback mechanisms
* Implemented stateless retrieval and LLM inference with latency monitoring and operational safeguards
* Containerized deployment with Docker, CI/CD with GitHub Actions, and production-grade code quality tools

---

## üé§ Interview Explanation (30 Seconds)

> "MineMEETS is an MLOps-focused multimodal RAG system. I built ingestion pipelines for text, audio, and images, generated embeddings with Whisper and CLIP, and indexed everything in Pinecone using meeting-scoped namespaces. The emphasis was on operational reliability ‚Äî reprocessing, metadata filtering, latency monitoring, and safe deletion ‚Äî rather than model experimentation. It's containerized, tested, and has CI/CD integrated."

**This answer demonstrates production ML engineering skills.**

---

## üìö Documentation

* [ARCHITECTURE.md](ARCHITECTURE.md) - Detailed system design and data flow
* [CONTRIBUTING.md](CONTRIBUTING.md) - Development guidelines
* [CHANGELOG.md](CHANGELOG.md) - Version history

---

## ü§ù Contributing

This is a portfolio project, but contributions are welcome:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests and linting (`make check`)
4. Commit your changes (`git commit -m 'Add amazing feature'`)
5. Push to the branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

---

## üìÑ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

* **Whisper** - OpenAI's speech recognition model
* **CLIP** - OpenAI's vision-language model
* **Pinecone** - Managed vector database
* **Ollama** - Local LLM runtime
* **Gradio** - ML interface framework

---

**Built with a focus on MLOps best practices, not research novelty.**
